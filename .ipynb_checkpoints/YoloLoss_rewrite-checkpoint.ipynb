{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import vocModel.nntools as nt\n",
    "import vocData as voc\n",
    "import vocModel\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = \"/datasets/ee285f-public/PascalVOC2012/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5717 5823\n"
     ]
    }
   ],
   "source": [
    "train_set = voc.VOCDetection_Yolo(dataset_root_dir,  image_set = 'train')\n",
    "val_set = voc.VOCDetection_Yolo(dataset_root_dir, image_set = 'val')\n",
    "print (len(train_set), len(val_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vocModel.YoloNet(7, 2, 20, 5,0.5)\n",
    "net = net.to(device)\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set,batch_size=3,shuffle=False,num_workers=0)\n",
    "train_iter = iter(train_loader)\n",
    "for i in range(1):\n",
    "    img,target = next(train_iter)\n",
    "    img,target = img.to(device), target.to(device)\n",
    "\n",
    "    output = net.forward(img)\n",
    "#     loss = net.criterion(output, target)\n",
    "#     print (loss)    \n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 30])\n",
      "tensor(20.4206, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = my_forward(output, target)\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_forward(pred, target):\n",
    " \n",
    "\n",
    "#     \"referce : https://github.com/thtrieu/darkflow/tree/master/darkflow/net (Tensor Flow)\"\n",
    "\n",
    "\n",
    "    loss=0\n",
    "\n",
    "    lamb_class_prob = 1\n",
    "    lamb_obj_conf = 1\n",
    "    lamb_noobj_conf = 0.5\n",
    "    lamb_coord = 5\n",
    "    \n",
    "    batch_size = y_pred.size(0)\n",
    "\n",
    "    \n",
    "    \n",
    "#     loss = 0\n",
    "#     n = pred.size()[0]\n",
    "#     object_mask = target[:,:,:,4] > 0\n",
    "#     object_mask = object_mask.unsqueeze(-1).expand_as(target)\n",
    "#     # 1. Classification loss   \n",
    "#     class_pred = pred[object_mask].view(-1,30)\n",
    "#     print (class_pred.size())\n",
    "#     class_pred = class_pred[:,20:]\n",
    "#     class_target = target[object_mask].view(-1,30)\n",
    "#     class_target = class_target[:,20:]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     class_loss = F.mse_loss(class_pred,class_target,reduction='sum')\n",
    "    \n",
    "#     # 2. Localization loss\n",
    "#     coords_pred = pred[]\n",
    "    \n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3114, 0.0603, 0.3071],\n",
      "        [0.2400, 0.2074, 0.1113],\n",
      "        [0.5634, 0.0395, 0.3035]])\n",
      "tensor([[1, 1, 1],\n",
      "        [0, 0, 0],\n",
      "        [1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((3,3))\n",
    "print (a)\n",
    "b = torch.from_numpy(np.array([[1,1,1],[0,0,0],[1,1,1]]))\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print (c.size()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5]) torch.Size([3, 4, 5]) (20, 5, 1) 94569426107440\n",
      "tensor([[[-0.0927, -0.0730,  0.3672,  1.3897, -0.5202],\n",
      "         [ 0.3496,  1.9922, -0.1502,  0.8792,  2.1860],\n",
      "         [ 0.8653,  0.9986,  0.6206, -0.3740,  0.6920],\n",
      "         [ 0.4099, -1.8327, -0.2812,  0.2164, -0.1108]],\n",
      "\n",
      "        [[ 0.2095,  0.6548, -0.9549, -1.3656, -0.5294],\n",
      "         [ 1.1057,  1.8712,  0.9089, -0.5541,  0.5597],\n",
      "         [ 1.2444,  0.6688,  1.0857,  2.3752, -0.3824],\n",
      "         [ 0.2090, -1.5950, -1.3451, -0.5951,  0.3794]],\n",
      "\n",
      "        [[-1.8272,  0.5864, -0.6422,  2.5283, -1.8432],\n",
      "         [-0.2214,  1.1106,  1.5708, -0.6046, -0.6657],\n",
      "         [ 0.2525,  0.6406, -0.6576, -0.1579, -1.3209],\n",
      "         [ 1.5862,  0.1400,  0.0511, -0.6695, -1.6985]]])\n",
      "torch.Size([4, 5, 3]) torch.Size([4, 5, 3]) (5, 1, 20) 94569426107440\n",
      "tensor([[[-0.0927,  0.2095, -1.8272],\n",
      "         [-0.0730,  0.6548,  0.5864],\n",
      "         [ 0.3672, -0.9549, -0.6422],\n",
      "         [ 1.3897, -1.3656,  2.5283],\n",
      "         [-0.5202, -0.5294, -1.8432]],\n",
      "\n",
      "        [[ 0.3496,  1.1057, -0.2214],\n",
      "         [ 1.9922,  1.8712,  1.1106],\n",
      "         [-0.1502,  0.9089,  1.5708],\n",
      "         [ 0.8792, -0.5541, -0.6046],\n",
      "         [ 2.1860,  0.5597, -0.6657]],\n",
      "\n",
      "        [[ 0.8653,  1.2444,  0.2525],\n",
      "         [ 0.9986,  0.6688,  0.6406],\n",
      "         [ 0.6206,  1.0857, -0.6576],\n",
      "         [-0.3740,  2.3752, -0.1579],\n",
      "         [ 0.6920, -0.3824, -1.3209]],\n",
      "\n",
      "        [[ 0.4099,  0.2090,  1.5862],\n",
      "         [-1.8327, -1.5950,  0.1400],\n",
      "         [-0.2812, -1.3451,  0.0511],\n",
      "         [ 0.2164, -0.5951, -0.6695],\n",
      "         [-0.1108,  0.3794, -1.6985]]])\n",
      "torch.Size([4, 5, 3]) torch.Size([4, 5, 3]) (15, 3, 1) 94569426098400\n",
      "tensor([[[-0.0927,  0.2095, -1.8272],\n",
      "         [-0.0730,  0.6548,  0.5864],\n",
      "         [ 0.3672, -0.9549, -0.6422],\n",
      "         [ 1.3897, -1.3656,  2.5283],\n",
      "         [-0.5202, -0.5294, -1.8432]],\n",
      "\n",
      "        [[ 0.3496,  1.1057, -0.2214],\n",
      "         [ 1.9922,  1.8712,  1.1106],\n",
      "         [-0.1502,  0.9089,  1.5708],\n",
      "         [ 0.8792, -0.5541, -0.6046],\n",
      "         [ 2.1860,  0.5597, -0.6657]],\n",
      "\n",
      "        [[ 0.8653,  1.2444,  0.2525],\n",
      "         [ 0.9986,  0.6688,  0.6406],\n",
      "         [ 0.6206,  1.0857, -0.6576],\n",
      "         [-0.3740,  2.3752, -0.1579],\n",
      "         [ 0.6920, -0.3824, -1.3209]],\n",
      "\n",
      "        [[ 0.4099,  0.2090,  1.5862],\n",
      "         [-1.8327, -1.5950,  0.1400],\n",
      "         [-0.2812, -1.3451,  0.0511],\n",
      "         [ 0.2164, -0.5951, -0.6695],\n",
      "         [-0.1108,  0.3794, -1.6985]]])\n",
      "torch.Size([3, 4, 5]) torch.Size([3, 4, 5]) (20, 5, 1) 94569426107440\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 4, 5)\n",
    "b = a.permute(1, 2, 0)\n",
    "c = b.contiguous()\n",
    "d = a.contiguous()\n",
    "# a has \"standard layout\" (also known as C layout in numpy) descending strides, and no memory gaps (stride(i-1) == size(i)*stride(i))\n",
    "print (a.size(), a.shape, a.stride(), a.data_ptr())\n",
    "print (a)\n",
    "# b has same storage as a (data_ptr), but has the strides and sizes swapped around\n",
    "print (b.size(), b.shape, b.stride(), b.data_ptr())\n",
    "print (b)\n",
    "# c is in new storage, where it has been arranged in standard layout (which is \"contiguous\")\n",
    "print (c.size(), c.shape, c.stride(), c.data_ptr())\n",
    "print (c)\n",
    "# d is exactly as a, as a was contiguous all along\n",
    "print (d.size(), d.shape, d.stride(), d.data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(pred_tensor,target_tensor):\n",
    "\n",
    "    N = pred_tensor.size()[0]\n",
    "    coo_mask = target_tensor[:,:,:,4] > 0\n",
    "    noo_mask = target_tensor[:,:,:,4] == 0\n",
    "    coo_mask = coo_mask.unsqueeze(-1).expand_as(target_tensor)\n",
    "    noo_mask = noo_mask.unsqueeze(-1).expand_as(target_tensor)\n",
    "\n",
    "    coo_pred = pred_tensor[coo_mask].view(-1,30)\n",
    "    box_pred = coo_pred[:,:10].contiguous().view(-1,5) #box[x1,y1,w1,h1,c1]\n",
    "    class_pred = coo_pred[:,10:]                       #[x2,y2,w2,h2,c2]\n",
    "\n",
    "    coo_target = target_tensor[coo_mask].view(-1,30)\n",
    "    box_target = coo_target[:,:10].contiguous().view(-1,5)\n",
    "    class_target = coo_target[:,10:]\n",
    "\n",
    "    # compute not contain obj loss\n",
    "    noo_pred = pred_tensor[noo_mask].view(-1,30)\n",
    "    noo_target = target_tensor[noo_mask].view(-1,30)\n",
    "    noo_pred_mask = torch.cuda.ByteTensor(noo_pred.size())\n",
    "    noo_pred_mask.zero_()\n",
    "    noo_pred_mask[:,4]=1;noo_pred_mask[:,9]=1\n",
    "    noo_pred_c = noo_pred[noo_pred_mask] #noo pred只需要计算 c 的损失 size[-1,2]\n",
    "    noo_target_c = noo_target[noo_pred_mask]\n",
    "    nooobj_loss = F.mse_loss(noo_pred_c,noo_target_c,size_average=False)\n",
    "\n",
    "    #compute contain obj loss\n",
    "    coo_response_mask = torch.cuda.ByteTensor(box_target.size())\n",
    "    coo_response_mask.zero_()\n",
    "    coo_not_response_mask = torch.cuda.ByteTensor(box_target.size())\n",
    "    coo_not_response_mask.zero_()\n",
    "    box_target_iou = torch.zeros(box_target.size()).cuda()\n",
    "    for i in range(0,box_target.size()[0],2): #choose the best iou box\n",
    "        box1 = box_pred[i:i+2]\n",
    "        box1_xyxy = torch.FloatTensor(box1.size())\n",
    "        box1_xyxy[:,:2] = box1[:,:2]/14. -0.5*box1[:,2:4]\n",
    "        box1_xyxy[:,2:4] = box1[:,:2]/14. +0.5*box1[:,2:4]\n",
    "        box2 = box_target[i].view(-1,5)\n",
    "        box2_xyxy = torch.FloatTensor(box2.size())\n",
    "        box2_xyxy[:,:2] = box2[:,:2]/14. -0.5*box2[:,2:4]\n",
    "        box2_xyxy[:,2:4] = box2[:,:2]/14. +0.5*box2[:,2:4]\n",
    "        iou = self.compute_iou(box1_xyxy[:,:4],box2_xyxy[:,:4]) #[2,1]\n",
    "        max_iou,max_index = iou.max(0)\n",
    "        max_index = max_index.data.cuda()\n",
    "\n",
    "        coo_response_mask[i+max_index]=1\n",
    "        coo_not_response_mask[i+1-max_index]=1\n",
    "\n",
    "        #####\n",
    "        # we want the confidence score to equal the\n",
    "        # intersection over union (IOU) between the predicted box\n",
    "        # and the ground truth\n",
    "        #####\n",
    "        box_target_iou[i+max_index,torch.LongTensor([4]).cuda()] = (max_iou).data.cuda()\n",
    "    box_target_iou = box_target_iou.cuda()\n",
    "    #1.response loss\n",
    "    box_pred_response = box_pred[coo_response_mask].view(-1,5)\n",
    "    box_target_response_iou = box_target_iou[coo_response_mask].view(-1,5)\n",
    "    box_target_response = box_target[coo_response_mask].view(-1,5)\n",
    "    contain_loss = F.mse_loss(box_pred_response[:,4],box_target_response_iou[:,4],size_average=False)\n",
    "    loc_loss = F.mse_loss(box_pred_response[:,:2],box_target_response[:,:2],size_average=False) +\\\n",
    "    F.mse_loss(torch.sqrt(box_pred_response[:,2:4]),torch.sqrt(box_target_response[:,2:4]),size_average=False)\n",
    "    #2.not response loss\n",
    "    box_pred_not_response = box_pred[coo_not_response_mask].view(-1,5)\n",
    "    box_target_not_response = box_target[coo_not_response_mask].view(-1,5)\n",
    "    box_target_not_response[:,4]= 0\n",
    "    #not_contain_loss = F.mse_loss(box_pred_response[:,4],box_target_response[:,4],size_average=False)\n",
    "\n",
    "    #I believe this bug is simply a typo\n",
    "    not_contain_loss = F.mse_loss(box_pred_not_response[:,4], box_target_not_response[:,4],size_average=False)\n",
    "\n",
    "    #3.class loss\n",
    "    class_loss = F.mse_loss(class_pred,class_target,size_average=False)\n",
    "    \n",
    "    return (self.l_coord*loc_loss + 2*contain_loss + not_contain_loss + self.l_noobj*nooobj_loss + class_loss)/N\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(box_a, box_b):\n",
    "    A = box_a.size(0)\n",
    "    B = box_b.size(0)\n",
    "    \n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
    "    \n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "def cal_iou(box_a, box_b):\n",
    "    inter = intersect(box_a, box_b)\n",
    "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
    "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
    "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
    "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
