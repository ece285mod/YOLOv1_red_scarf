{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import nntools as nt\n",
    "import vocData as voc\n",
    "import vocModel\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = \"/datasets/ee285f-public/PascalVOC2012/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5717 5823\n"
     ]
    }
   ],
   "source": [
    "train_set = voc.VOCDetection_Yolo(dataset_root_dir,  image_set = 'train')\n",
    "val_set = voc.VOCDetection_Yolo(dataset_root_dir, image_set = 'val')\n",
    "print (len(train_set), len(val_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 448, 448]) torch.Size([3, 7, 7, 30])\n",
      "torch.Size([3, 3, 448, 448]) torch.Size([3, 7, 7, 30])\n",
      "torch.Size([3, 3, 448, 448]) torch.Size([3, 7, 7, 30])\n",
      "torch.Size([3, 3, 448, 448]) torch.Size([3, 7, 7, 30])\n",
      "torch.Size([3, 3, 448, 448]) torch.Size([3, 7, 7, 30])\n",
      "torch.Size([3, 3, 448, 448]) torch.Size([3, 7, 7, 30])\n",
      "torch.Size([3, 3, 448, 448]) torch.Size([3, 7, 7, 30])\n",
      "torch.Size([3, 3, 448, 448]) torch.Size([3, 7, 7, 30])\n",
      "torch.Size([3, 3, 448, 448]) torch.Size([3, 7, 7, 30])\n",
      "torch.Size([3, 3, 448, 448]) torch.Size([3, 7, 7, 30])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set,batch_size=3,shuffle=False,num_workers=0)\n",
    "train_iter = iter(train_loader)\n",
    "for i in range(10):\n",
    "    img,target = next(train_iter)\n",
    "    print(img.size(),target.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vocModel.YoloNet(7, 2, 20, 5,0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'vocModel' has no attribute 'ObjectDetectingStatsManager'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a88cb9af9c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0madam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstats_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectDetectingStatsManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m exp1 = nt.Experiment(net, train_set, val_set, adam, stats_manager,batch_size=4,\n\u001b[1;32m      7\u001b[0m                      output_dir=\"test3\", perform_validation_during_training=False)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'vocModel' has no attribute 'ObjectDetectingStatsManager'"
     ]
    }
   ],
   "source": [
    "net = vocModel.YoloNet(7, 2, 20, 5,0.5)\n",
    "net = net.to(device)\n",
    "lr = 1e-3\n",
    "adam = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "stats_manager = vocModel.DetectionStatsManager()\n",
    "exp1 = nt.Experiment(net, train_set, val_set, adam, stats_manager,batch_size=4,\n",
    "                     output_dir=\"test3\", perform_validation_during_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.slideshare.net/TaegyunJeon1/pr12-you-only-look-once-yolo-unified-realtime-object-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://arxiv.org/pdf/1506.02640.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/xiongzihua/pytorch-YOLO-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/happyjin/pytorch-YOLO/blob/master/YoloLoss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class YoloNet_IMAGENET(nn.Module):\n",
    "    def __init__(self):\n",
    "        self._initialize_weights()\n",
    "        self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "        self.conv2 = BasicConv2d(64, 192, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "        \n",
    "        self.red3 = nn.Sequential(\n",
    "            ReductionLayer(192, 128, 256),\n",
    "            ReductionLayer(256, 256, 512)\n",
    "        )\n",
    "        self.maxpool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "\n",
    "        self.red4 = nn.Sequential(\n",
    "            ReductionLayer(512, 256, 512),\n",
    "            ReductionLayer(512, 256, 512),\n",
    "            ReductionLayer(512, 256, 512),\n",
    "            ReductionLayer(512, 256, 512),\n",
    "            ReductionLayer(512, 512, 1024)\n",
    "        )\n",
    "        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
    "        \n",
    "        self.red5 = nn.Sequential(\n",
    "            ReductionLayer(1024, 512, 1024),\n",
    "            ReductionLayer(1024, 512, 1024),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(1024, 1000)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.red3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.red4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.red5(x)\n",
    "        x = self.avgpool(x)\n",
    "        # N x 1024 x 1 x 1\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # N x 1024\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                import scipy.stats as stats\n",
    "                X = stats.truncnorm(-2, 2, scale=0.01)\n",
    "                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
    "                values = values.view(m.weight.size())\n",
    "                with torch.no_grad():\n",
    "                    m.weight.copy_(values)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReductionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, ch3x3red, ch3x3):\n",
    "        super(ReductionLayer, self).__init__()\n",
    "\n",
    "        self.red = nn.Sequential(\n",
    "            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.red(x)\n",
    "        return output\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "        import scipy.stats as stats\n",
    "        X = stats.truncnorm(-2, 2, scale=0.01)\n",
    "        values = torch.as_tensor(X.rvs(self.conv.weight.numel()), dtype=self.conv.weight.dtype)\n",
    "        values = values.view(self.conv.weight.size())\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.leaky_relu(x, 0.1, inplace=True)\n",
    "#         return F.rrelu(x, inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
