{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "import vocModel.nntools as nt\n",
    "import vocData as voc\n",
    "import vocModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = \"/datasets/ee285f-public/PascalVOC2012/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'vocData' has no attribute 'VOCDetection_Yolo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e43a51c9a5e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVOCDetection_Yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root_dir\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mimage_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVOCDetection_Yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_root_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'vocData' has no attribute 'VOCDetection_Yolo'"
     ]
    }
   ],
   "source": [
    "train_set = voc.VOCDetection_Yolo(dataset_root_dir,  image_set = 'train')\n",
    "val_set = voc.VOCDetection_Yolo(dataset_root_dir, image_set = 'val')\n",
    "print (len(train_set), len(val_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(exp, fig, axes):\n",
    "    axes.clear()\n",
    "    axes.plot([exp.history[k]['loss'] for k in range(exp.epoch)],\n",
    "                 label=\"traininng loss\")\n",
    "\n",
    "    axes.legend()\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vocModel.YoloNet(7, 2, 20, 5,0.5)\n",
    "net = net.to(device)\n",
    "lr = 1e-3\n",
    "# adam = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "stats_manager = vocModel.DetectionStatsManager()\n",
    "exp1 = nt.Experiment(net, train_set, val_set, optimizer, stats_manager,batch_size=4,\n",
    "                     output_dir=\"data/test5\", perform_validation_during_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=1, figsize=(5, 5))\n",
    "exp1.run(num_epochs=13, plot=lambda exp: plot(exp, fig=fig, axes=axes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {'person' :0, 'bird' : 1, 'cat' : 2, 'cow': 3, 'dog': 4, 'horse' : 5, 'sheep' : 6,\n",
    "                  'aeroplane' :7, 'bicycle' :8, 'boat' :9, 'bus':10, 'car':11, 'motorbike' :12, 'train':13,\n",
    "                  'bottle' :14, 'chair':15, 'diningtable':16, 'pottedplant':17, 'sofa': 18, 'tvmonitor':19}\n",
    "class_list = list(class_dict)\n",
    "\n",
    "color_list = ['b', 'g', 'c', 'm', 'y', 'k', 'w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_transform(image, ax=plt):\n",
    "    normalize = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "                                 std=[1/0.229, 1/0.224, 1/0.225])    \n",
    "    image = np.array(normalize(image).numpy())\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(val_set,batch_size=1,shuffle=False,num_workers=0)\n",
    "test_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 7\n",
    "B = 2\n",
    "w = 448\n",
    "h = 448\n",
    "img,(bboxes, labels, target)= next(test_iter)\n",
    "\n",
    "img = img.to(device)\n",
    "\n",
    "output = net.forward(img)\n",
    "\n",
    "img = img.to('cpu')\n",
    "\n",
    "output = output.to('cpu')\n",
    "target = output[0]\n",
    "\n",
    "print (target.shape)\n",
    "# target = target[0]\n",
    "\n",
    "cell_size = 1./C\n",
    "\n",
    "fig, axs = plt.subplots(ncols=1, figsize=(5, 5))\n",
    "\n",
    "white = (255,255,255)\n",
    "light_blue = (255,200,100)\n",
    "green = (0,255,0)\n",
    "light_red = (30,30,255)\n",
    "\n",
    "bboxes = bboxes[0]*448.0\n",
    "labels = labels[0]\n",
    "img_mod = image_transform(img[0])\n",
    "\n",
    "for i in range(bboxes.size()[0]):\n",
    "    xmin, ymin, xmax, ymax = bboxes[i][0].item(), bboxes[i][1].item(), bboxes[i][2].item(), bboxes[i][3].item()\n",
    "    rect = patches.Rectangle((xmin,ymin),xmax-xmin,ymax-ymin,linewidth=4,edgecolor='r',facecolor='none')\n",
    "    axs.add_patch(rect)\n",
    "\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        \n",
    "        maxi = torch.max(target[i][j][4], target[i][j][9])\n",
    "        max_idx = (target[i][j][9] > target[i][j][4]).item()\n",
    "        t = torch.argmax(target[i][j][10:]).item()\n",
    "\n",
    "#         print (maxi*target[i][j][10+t])\n",
    "        if maxi > 0.3:\n",
    "            w, h= target[i][j][2 + max_idx*5]**2, target[i][j][3 + max_idx*5]**2\n",
    "            x_c, y_c =  target[i][j][max_idx*5], target[i][j][1 + max_idx*5]\n",
    "            xmin = cell_size * (j + x_c) - w/2\n",
    "            ymin = cell_size * (i + y_c) - h/2\n",
    "            \n",
    "            xmax = xmin + w\n",
    "            ymax = ymin + h\n",
    "            \n",
    "            xmin, ymin, xmax, ymax = xmin.item()*448.0, ymin.item()*448.0\\\n",
    "            , xmax.item()*448.0, ymax.item()*448.0\n",
    "            print (xmin, ymin, xmax, ymax, class_list[t], target[i][j][10+t],maxi)\n",
    "            color = color_list[t % len(color_list)]\n",
    "            rect = patches.Rectangle((xmin,ymin),xmax-xmin,ymax-ymin,linewidth=2,edgecolor=color,facecolor='none')\n",
    "            axs.add_patch(rect)\n",
    "\n",
    "axs.axis('off')\n",
    "axs.imshow(img_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/happyjin/pytorch-YOLO/blob/master/YoloLoss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/xiongzihua/pytorch-YOLO-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/Cartucho/mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttps://github.com/tjwhenderson/Multi-Object-Detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
