{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision as tv\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "import vocModel.nntools as nt\n",
    "import vocData as voc\n",
    "import vocModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = \"/datasets/ee285f-public/PascalVOC2012/\"\n",
    "lr = 1e-3\n",
    "batch_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_data_loader():\n",
    "    dataset = voc.VOCDetection(dataset_root_dir,  image_set = 'trainval')\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    validation_split = 0.1\n",
    "    test_split = 0.1\n",
    "    val_split = int(np.floor(validation_split * dataset_size))\n",
    "    test_split = int(np.floor(test_split * dataset_size))\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices, val_indices, test_indices = indices[val_split+test_split:],\\\n",
    "    indices[:val_split], indices[val_split:val_split+test_split]\n",
    "\n",
    "    print (len(train_indices) + len(val_indices) + len(test_indices))\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "    test_sampler =  SubsetRandomSampler(test_indices)\n",
    "    # Define data loaders\n",
    "    print (batch_size)\n",
    "    train_loader = td.DataLoader(dataset, batch_size=batch_size, \\\n",
    "                                 sampler=train_sampler,\\\n",
    "                                 drop_last=True, pin_memory=True)\n",
    "\n",
    "    val_loader = td.DataLoader(dataset, batch_size=batch_size,\\\n",
    "                               sampler=valid_sampler, \\\n",
    "                               drop_last=True, pin_memory=True)\n",
    "\n",
    "    test_loader = td.DataLoader(dataset, batch_size=1,\\\n",
    "                               sampler=test_sampler, \\\n",
    "                               drop_last=False, pin_memory=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = voc_data_loader()\n",
    "print (\"Dataset is divided into train :{}, val : {}, test :{} \".format(len(train_loader), len(val_loader), len(test_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(exp, fig, axes):\n",
    "    axes.clear()\n",
    "    axes.plot([exp.history[k][0]['loss'] for k in range(exp.epoch)],\n",
    "                 label=\"traininng loss\")\n",
    "    axes.plot([exp.history[k][1]['loss'] for k in range(exp.epoch)],\n",
    "                 label=\"validation loss\")\n",
    "    \n",
    "    axes.set\n",
    "    axes.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = vocModel.YoloNet(7, 2, 20, 5,0.5)\n",
    "net = vocModel.YoloNet_res(7, 2, 20, 5,0.5)\n",
    "net = net.to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay = 5*1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20,40,60], gamma=0.1)\n",
    "stats_manager = vocModel.DetectionStatsManager()\n",
    "exp1 = nt.Experiment(net, train_loader, val_loader, scheduler, stats_manager,batch_size=batch_size,\n",
    "                     output_dir=\"data/resnet_newYOLOLOSS_2\", perform_validation_during_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=1, figsize=(5, 5))\n",
    "exp1.run(num_epochs=80, plot=lambda exp: plot(exp, fig=fig, axes=axes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {'person' :0, 'bird' : 1, 'cat' : 2, 'cow': 3, 'dog': 4, 'horse' : 5, 'sheep' : 6,\n",
    "                  'aeroplane' :7, 'bicycle' :8, 'boat' :9, 'bus':10, 'car':11, 'motorbike' :12, 'train':13,\n",
    "                  'bottle' :14, 'chair':15, 'diningtable':16, 'pottedplant':17, 'sofa': 18, 'tvmonitor':19}\n",
    "class_list = list(class_dict)\n",
    "\n",
    "color_list = ['b', 'g', 'c', 'm', 'y', 'k', 'w']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot single image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "predictions = []\n",
    "for i in range(len(class_dict)):\n",
    "    predictions.append([])\n",
    "print (predictions)\n",
    "gt_class_count = torch.zeros(len(class_dict)).to(device)\n",
    "print (gt_class_count)\n",
    "test_iter = iter(test_loader)\n",
    "print (len(test_loader))\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "#     for img, target, bboxes, labels in test_loader:\n",
    "    for t in range(1):\n",
    "        img, target, bboxes, labels= next(test_iter)\n",
    "        img, target, bboxes, labels= img.to(device).detach()\\\n",
    "        , target.to(device).detach(), bboxes.to(device).detach(), labels.to(device).detach()\n",
    "\n",
    "        output = net.forward(img)\n",
    "        output = output[0]\n",
    "        target = target[0]\n",
    "        bboxes = bboxes[0]\n",
    "        labels = labels[0]\n",
    "\n",
    "        # undo normailization\n",
    "        img_mod = image_transform(img[0].to('cpu'))\n",
    "\n",
    "        #gt variables\n",
    "        gt_bboxes, gt_labels = from_target_to_bndboxes(bboxes, labels)\n",
    "        \n",
    "        # output to bndboxes\n",
    "        output_bboxes, output_labels = from_prediction_to_bndboxes(output, conf_threshold)\n",
    "\n",
    "        #plot image\n",
    "        fig, axs = plt.subplots(ncols=1, figsize=(5, 5))\n",
    "        plot_single_image(output_bboxes, output_labels, gt_bboxes, gt_labels, axs)\n",
    "        axs.axis('off')\n",
    "        axs.imshow(img_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_transform(image, ax=plt):\n",
    "    normalize = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "                                 std=[1/0.229, 1/0.224, 1/0.225])    \n",
    "    image = np.array(normalize(image).numpy())\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1])\n",
    "    return image\n",
    "\n",
    "\n",
    "def plot_single_image(output_bboxes, output_class_idx, gt_bboxes, gt_labels, axs) : \n",
    "    for i in range(gt_bboxes.size()[0]):\n",
    "        xmin, ymin, xmax, ymax = gt_bboxes[i,:4]\n",
    "        print (\"GT\",i,gt_bboxes[i,:].to('cpu').numpy(), class_list[gt_labels[i]])\n",
    "        rect = patches.Rectangle((xmin,ymin),xmax-xmin,ymax-ymin,linewidth=4,edgecolor='r',facecolor='none')\n",
    "        axs.add_patch(rect)\n",
    "\n",
    "    for i in range(output_bboxes.size()[0]):\n",
    "        xmin, ymin, xmax, ymax = output_bboxes[i,:4]\n",
    "        t = output_class_idx[i]\n",
    "        print (output_bboxes[i,:].to('cpu').numpy(), class_list[t])\n",
    "        color = color_list[t % len(color_list)]\n",
    "        rect = patches.Rectangle((xmin,ymin),xmax-xmin,ymax-ymin,linewidth=2,edgecolor=color,facecolor='none')\n",
    "        axs.add_patch(rect)\n",
    "\n",
    "    \n",
    "def from_target_to_bndboxes(bboxes, labels):\n",
    "\n",
    "    mask = bboxes[:,3]>0.0\n",
    "    \n",
    "    gt_bboxes = bboxes[mask]*448.0\n",
    "    gt_labels = labels[mask]\n",
    "\n",
    "    return gt_bboxes, gt_labels.view(-1,1).byte()\n",
    "\n",
    "    \n",
    "    \n",
    "def from_prediction_to_bndboxes(output, conf_threshold):\n",
    "    \n",
    "    output_bboxes =  output.view(-1, 30)[:,:10]\n",
    "    maxi = torch.eq(output_bboxes[:,[4,9]], torch.max(output_bboxes[:,[4,9]], dim =1)[0].unsqueeze(1)).view(-1)\n",
    "    output_bboxes = output_bboxes.contiguous().view(-1,5)[maxi]\n",
    "    output_class_prob, output_labels = torch.max(output.view(-1, 30)[:,10:], dim = 1)\n",
    "    output_bboxes[:,4] *= output_class_prob\n",
    "\n",
    "    # calculate bndboxes to xmin, ymin, xmax, ymax\n",
    "    cell_size = 1./7.0\n",
    "\n",
    "    wh= torch.pow(output_bboxes[:,[2,3]], 2)\n",
    "    ij = torch.from_numpy(np.array([ (i%C, i//C) for i in range(C*C)])).to(device).float()\n",
    "    xy_center = (output_bboxes[:,[0,1]] + ij)* cell_size\n",
    "    xy_min = xy_center - wh/2\n",
    "    xy_min = torch.max(xy_min, torch.zeros_like(xy_min)) * 448.0\n",
    "    xy_max = xy_center + wh/2\n",
    "    xy_max = torch.min(xy_max, torch.ones_like(xy_min)) * 448.0\n",
    "    output_bboxes[:,:4] = torch.cat((xy_min,  xy_max), dim = 1)\n",
    "\n",
    "    # Choose high confidence bnd boxes\n",
    "    conf_selected =output_bboxes[:,4] > conf_threshold\n",
    "    output_bboxes = output_bboxes[conf_selected]\n",
    "    output_labels = output_labels[conf_selected]\n",
    "    \n",
    "    return output_bboxes, output_labels.view(-1,1).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
